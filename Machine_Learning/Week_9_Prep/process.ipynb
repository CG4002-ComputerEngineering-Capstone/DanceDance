{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0fd6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sn\n",
    "from scipy.signal import medfilt\n",
    "from scipy import stats\n",
    "plt.style.use('ggplot')\n",
    "sn.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3bf5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_FREQ = 20 # Hz \n",
    "WINDOW_SIZE = 2 # sec \n",
    "OVERLAP = 20 # \n",
    "SEGMENT_SIZE = SAMPLING_FREQ * WINDOW_SIZE # 40\n",
    "\n",
    "SENSOR_COLS = [\"acc_X\", \"acc_Y\", \"acc_Z\", \"gyro_X\", \"gyro_Y\", \"gyro_Z\", \"yaw\", \"pitch\", \"roll\"]\n",
    "\n",
    "DANCE_TO_NUM_MAP = {'dab': 0, 'jamesbond': 1, 'mermaid': 2}\n",
    "# NUM_TO_DANCE_MAP = {0: 'dab', 1: 'jamesbond', 2: 'mermaid'}\n",
    "\n",
    "TRAIN_FILEPATH = \"./capstone_data/train/*.csv\"\n",
    "TEST_FILEPATH = \"./capstone_data/test/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f03601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_paths(location):\n",
    "    \"\"\"\n",
    "    Gets file path to each csv data file.\n",
    "    Input: filepath to csv files i.e. string\n",
    "    Return: 1D array of filepath to each csv which contains sensor data for each trial by a subject for a dance move \n",
    "    \"\"\"\n",
    "    data_paths = []\n",
    "    for name in glob.glob(location):\n",
    "        data_paths.append(name)\n",
    "    return data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e236cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliseTrainData(dframe):\n",
    "    \"\"\"\n",
    "    Normalize features for data set (values between -1 and 1). Columns rounded to 4dp after normalisation.\n",
    "    Input: raw sensor dframe \n",
    "    Return: training set max values for each col packaged in a dict \n",
    "    \"\"\"\n",
    "    \n",
    "    global SENSOR_COLS\n",
    "    \n",
    "    train_max = {}\n",
    "    \n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    for col in SENSOR_COLS:\n",
    "        dframe[col] = dframe[col].div(100).round(6)\n",
    "        train_max[col] = dframe[col].max()\n",
    "        dframe[col] = dframe[col] / dframe[col].max()\n",
    "        dframe[col] = dframe[col].round(4)\n",
    "    return train_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f17864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliseTestData(dframe, train_max):\n",
    "    \"\"\"\n",
    "    Normalize features for data set (values between -1 and 1). Columns rounded to 4dp after normalisation.\n",
    "    Input: raw sensor dframe \n",
    "    No return value\n",
    "    \"\"\"\n",
    "    \n",
    "    global SENSOR_COLS\n",
    "    \n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    for col in SENSOR_COLS:\n",
    "        dframe[col] = dframe[col].div(100).round(6)\n",
    "        dframe[col] = dframe[col] / train_max[col]\n",
    "        dframe[col] = dframe[col].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e40d9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_signal(df):\n",
    "    \"\"\"\n",
    "    Applies 3rd order median filter for each signal i.e. Each axial column in dataset.\n",
    "    Input: raw_df\n",
    "    \"\"\"\n",
    "    global SENSOR_COLS\n",
    "    \n",
    "    for col in SENSOR_COLS: \n",
    "        array = np.array(df[col]) \n",
    "        med_filtered = medfilt(array, kernel_size=3) \n",
    "        df[col] = med_filtered  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431be434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenator(raw_dic):\n",
    "    \"\"\"\n",
    "    Concatenate raw dict along the rows to generate raw train and raw test dframes.\n",
    "    Input: raw_dic\n",
    "    Return: concatenated_df i.e. dframe \n",
    "    \"\"\"\n",
    "    concatenated_df = pd.concat(raw_dic.values(), axis = 0, ignore_index=True)\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd37f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rawData(given_filepaths):\n",
    "    \"\"\"\n",
    "    Generate training and test dataframes from raw sensor data packaged into dict.\n",
    "    Input: given_filepaths i.e. filepaths 1D array\n",
    "    Return: dictionary of raw dfs, with key being {subjectName}_{dance}_{trialNum}\n",
    "    \"\"\"\n",
    "    global SENSOR_COLS\n",
    "    frames = {}\n",
    "    for filepath in given_filepaths:\n",
    "        _, s, subjectName, ext = filepath.split(\"_\")\n",
    "        _, _, dance = s.split(\"/\")\n",
    "        trialNum, _ = ext.split(\".\")\n",
    "        raw_df = pd.read_csv(filepath, names=SENSOR_COLS, index_col=None)\n",
    "        raw_df.dropna(inplace= True)\n",
    "        raw_df.reset_index(drop=True,inplace=True)\n",
    "        raw_df[\"subject\"] = subjectName\n",
    "        raw_df[\"trialNum\"] = int(trialNum)\n",
    "        raw_df[\"dance\"] = dance\n",
    "        frames[f\"{subjectName}_{dance}_{trialNum}\"] = raw_df\n",
    "    return frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eea990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_df(df, targetCol):\n",
    "    \"\"\"\n",
    "    Segment df into sliding windows of values.\n",
    "    Input: df i.e. concatenated mega df \n",
    "    Input: targetCol i.e. labels col for dance moves \n",
    "    Returns: 3D Numpy Array representing Windows of values and the corresponding labels \n",
    "    \"\"\"\n",
    "    global SENSOR_COLS, SEGMENT_SIZE, OVERLAP\n",
    "    \n",
    "    segments = []\n",
    "    labels = []\n",
    "    # In each iteration, the row jumps by the overlap size\n",
    "    # grab all rows of feature column values corresponding to length of segment \n",
    "    # grab corresponding mode of targetCol\n",
    "    for row in range(0, len(df) - SEGMENT_SIZE, OVERLAP):\n",
    "        window = []\n",
    "        for col in SENSOR_COLS: \n",
    "            window.append(df[col][row:row+SEGMENT_SIZE].values)\n",
    "            \n",
    "        segments.append(window)\n",
    "        label = stats.mode(df[targetCol][row:row+SEGMENT_SIZE])[0][0]\n",
    "        labels.append(label)\n",
    "        \n",
    "    reshaped_segments = np.asarray(segments,dtype =np.float32).reshape(-1,SEGMENT_SIZE,len(SENSOR_COLS))\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    # reshaped_segments will be x and labels will be y \n",
    "    return reshaped_segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87501d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputVector(reshapedSegments):\n",
    "    \"\"\"\n",
    "    Get the input vector to be fed into nn.\n",
    "    Input: reshapedSegments after segmentation of df \n",
    "    Return: Input vector of shape n windows * (20*1*12)\n",
    "    Note: num of windows, n = (len(df) / overlap) - 1, if you take first window as w1, else it will be (len(df) / overlap) - 2\n",
    "    \"\"\"\n",
    "    global SAMPLING_FREQ, WINDOW_SIZE, SENSOR_COLS\n",
    "    \n",
    "    num_of_input_features = SAMPLING_FREQ * WINDOW_SIZE * len(SENSOR_COLS)\n",
    "    inputVector = reshapedSegments.reshape(reshapedSegments.shape[0], num_of_input_features)\n",
    "    \n",
    "    return inputVector.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d9c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.grid(False)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ff6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df = concatenator(gen_rawData(load_data_paths(TRAIN_FILEPATH)))\n",
    "raw_test_df = concatenator(gen_rawData(load_data_paths(TEST_FILEPATH)))\n",
    "filter_signal(raw_train_df)\n",
    "filter_signal(raw_test_df)\n",
    "TRAIN_MAX = normaliseTrainData(raw_train_df)\n",
    "normaliseTestData(raw_test_df, TRAIN_MAX)\n",
    "raw_test_df[\"target\"] = raw_test_df[\"dance\"].map(DANCE_TO_NUM_MAP)\n",
    "raw_train_df[\"target\"] = raw_train_df[\"dance\"].map(DANCE_TO_NUM_MAP)\n",
    "test_segs, lbl_test = segment_df(raw_test_df, \"target\")\n",
    "train_segs, lbl_train = segment_df(raw_train_df, \"target\")\n",
    "training_X = getInputVector(train_segs)\n",
    "testing_X = getInputVector(test_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc659aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingVector shape (2698, 360)\n",
      "Label Train shape (2698,)\n"
     ]
    }
   ],
   "source": [
    "print(\"TrainingVector shape\", training_X.shape)\n",
    "print(\"Label Train shape\", lbl_train.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e976dee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestingVector shape (538, 360)\n",
      "Label Test shape (538,)\n"
     ]
    }
   ],
   "source": [
    "print(\"TestingVector shape\", testing_X.shape)\n",
    "print(\"Label Test shape\", lbl_test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b38b43e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc_X': 1.27,\n",
       " 'acc_Y': 1.27,\n",
       " 'acc_Z': 1.27,\n",
       " 'gyro_X': 276.28,\n",
       " 'gyro_Y': 266.44,\n",
       " 'gyro_Z': 271.8,\n",
       " 'yaw': 327.28,\n",
       " 'pitch': 15.59,\n",
       " 'roll': 15.14}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_MAX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
